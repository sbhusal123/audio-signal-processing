{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea81189-163c-4c7a-a7d8-6110b1e8975d",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Chapter 5: Windowing and Framing\n",
    "\n",
    "## âœ¨ Why Windowing is Needed\n",
    "\n",
    "- Real-world signals (like audio) **change over time** â€” they are **non-stationary.**\n",
    "- Fourier Transform assumes the signal is **infinite** and **unchanging** â€” not true for real audio.\n",
    "- **Windowing** means analyzing small chunks where the signal is almost stationary.\n",
    "- **Without windowing**, you would mix information from **different times**, causing **spectral leakage** (wrong frequency analysis).\n",
    "\n",
    "**Goal:**\n",
    "- âœ… Focus on a small region of the signal\n",
    "- âœ… Reduce leakage\n",
    "- âœ… Prepare for frame-based audio analysis (like in STFT, MFCCs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e40be8e-fd49-49e5-a2c0-e2e7b2b1a6ec",
   "metadata": {},
   "source": [
    "## âœ¨ What is a Window Function?\n",
    "\n",
    "- A **window function** is just a **shaped curve** (usually tapering at edges) that you multiply your signal chunk by.\n",
    "\n",
    "- It **smoothly reduces** the ends of the chunk to **zero**, **avoiding sharp jumps.**\n",
    "\n",
    "\n",
    "## Common Window Functions:\n",
    "\n",
    "| Window Name | Purpose                            | Formula                       |\n",
    "|:-----------:|:----------------------------------:|:-----------------------------:|\n",
    "| **Hamming** | Reduce side lobes, moderate leakage | $0.54 - 0.46 \\cos\\left(\\frac{2\\pi n}{N}\\right) $ |\n",
    "| **Hann**    | Even lower leakage, symmetric      | $ 0.5 \\left(1 - \\cos\\left(\\frac{2\\pi n}{N}\\right)\\right)$ |\n",
    "| **Blackman**| Even stronger tapering\n",
    "\n",
    "- ðŸ”µ **Hamming** is often used in speech/audio processing.\n",
    "- ðŸ”µ **Hann** is common for music signals.\n",
    "- ðŸ”µ **Blackman** gives better frequency separation, but wider main lobe\n",
    "\n",
    "\n",
    "\n",
    "## âœ¨ What is Framing?\n",
    "- **Frame:** a small segment of the signal (e.g., 25ms).\n",
    "- **Hop size:** how much you move to start the next frame (e.g., 10ms).\n",
    "- This creates **overlapping frames.**\n",
    "\n",
    "| **Concept** | **Typical Values**                   |\n",
    "|:-----------:|:------------------------------------:|\n",
    "|  Frame Size |20-40 ms (eg. 25ms)|\n",
    "|  Hop Size |10 ms (eg. 50% overlap)|\n",
    "\n",
    "- âœ… Important for speech and music analysis.\n",
    "- âœ… Makes sure you don't miss important changes happening between frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf99223d-692d-47de-bbcb-5911a5d2b06a",
   "metadata": {},
   "source": [
    "# ðŸ§ª Practical Exercises\n",
    "## ðŸ“Œ 1. Split an Audio Signal into Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db0b193-3615-4f18-9950-4dea67c3ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def frame_signal(signal, frame_size, hop_size, sampling_rate):\n",
    "    frame_len = int(frame_size * sampling_rate)\n",
    "    hop_len = int(hop_size * sampling_rate)\n",
    "    num_frames = 1 + (len(signal) - frame_len) // hop_len\n",
    "\n",
    "    frames = np.zeros((num_frames, frame_len))\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        start = i * hop_len\n",
    "        frames[i, :] = signal[start:start + frame_len]\n",
    "    \n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab26232-a27f-499d-93c8-e047ac61e456",
   "metadata": {},
   "source": [
    "## Usage Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d76e443-93c0-4cd9-bf08-4274fbb9ccd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames: 198\n",
      "Each frame length: 400 samples\n"
     ]
    }
   ],
   "source": [
    "# Assume 'data' and 'sampling_rate' are already loaded\n",
    "frame_size = 0.025  # 25 ms\n",
    "hop_size = 0.010    # 10 ms\n",
    "\n",
    "# Settings\n",
    "sampling_rate = 16000  # 16 kHz\n",
    "duration = 2.0         # 2 seconds\n",
    "t = np.linspace(0, duration, int(sampling_rate * duration), endpoint=False)\n",
    "data = 0.6 * np.sin(2 * np.pi * 440 * t) + 0.3 * np.sin(2 * np.pi * 880 * t) + 0.05 * np.random.normal(0, 1, t.shape)\n",
    "\n",
    "\n",
    "frames = frame_signal(data, frame_size, hop_size, sampling_rate)\n",
    "print(f\"Number of frames: {frames.shape[0]}\")\n",
    "print(f\"Each frame length: {frames.shape[1]} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc80d4-f6cf-4a0e-8357-645b624e54b1",
   "metadata": {},
   "source": [
    "- **What this gives you:**\n",
    "    - A **clean tone at 440 Hz** (like \"A\" musical note)\n",
    "    - A **higher harmonic at 880 Hz**\n",
    "    - **Tiny random noise** to simulate real-world signals\n",
    "\n",
    "- **Perfect for:**\n",
    "    - Testing **low-pass, high-pass, band-pass** filters\n",
    "    - Practicing **frame splitting** and **windowing**\n",
    "    - Practicing **STFT, spectrograms,** etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9302fc44-2955-433b-8f34-950ff3521304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
